{
 "cells": [
  {
   "source": [
    "## Merge cleaned SSO 2018-2021 concert data with composer and conductor metadata from Wikipedia and Wikidata\n",
    "\n",
    "This code is in Jupyter notebook format to enable more flexibility in executing different sections as needed.\n",
    "\n",
    "(It would probably be more efficient to split some of this code into separate modules, but I haven't had the time or motivation to do so yet)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, element\n",
    "from collections import namedtuple\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    "from qwikidata.sparql import (get_subclasses_of_item, return_sparql_query_results)\n",
    "from sso_utilities import file_utils\n",
    "from typing import Dict, List, NamedTuple, Tuple, TypedDict\n",
    "from unidecode import unidecode\n",
    "\n",
    "import mwparserfromhell as mwparser\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import unicodedata\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "source": [
    "### RULES USED FOR COMPOSER/CONDUCTOR NATIONALITY CLASSIFICATION ###\n",
    "* For purposes of (imperfect) simplification:\n",
    "  * 'Bohemian' composers are counted as 'Czech'. Don't @ me.\n",
    "  * Certain double-listed Austrian/German composers (e.g. Mozart and Schubert) are counted as 'Austrian'.\n",
    "  * Composers of disputed nationality (e.g. Gluck) are counted as the nationality most widely accepted in authoritative sources such as the New Grove Dictionary of Music.\n",
    "* Other European composers should be counted as either (a) the nationality assigned them in Grove Dictionary (b) or failing that (ambiguity and/or non-existence in Grove), the nationality whose language they primarily composed in (if opera/vocal) - e.g. Offenbach counted as 'French' - or whose country in which they spent the majority of their professional life.\n",
    "* Soviet composers e.g. Khachaturian are counted as their ethnic nationality rather than 'Russian' or 'Soviet'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Wikidata and related utility methods\n",
    "\n",
    "Methods for querying and manipulating conductor and composer metadata from Wikidata and merging it with Wikipedia and SSO data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_sso_composers(composers: pd.Series) -> List[str]:\n",
    "    \"\"\"Deduplicate SSO composers.\n",
    "\n",
    "    :param composers: List of composers from cleaned SSO data\n",
    "    \"\"\"\n",
    "    deduped_composers = sorted(set(reduce(lambda x, y: x + y, composers)))\n",
    "    deduped_composers = [ unicodedata.normalize('NFC', composer) for composer in deduped_composers if composer != 'Unknown' ]\n",
    "    return deduped_composers\n",
    "\n",
    "def impute_composer_gender(df_row: pd.Series) -> pd.Series:\n",
    "    \"\"\"Update SSO composer gender value with the corresponding Wikidata value if the Wikidata value is not 'Unknown'.\n",
    "    Assumes Wikidata to always be the authoritative source.\n",
    "\n",
    "    :param df_row: Row from merged SSO composer map and Wikidata composer query response\n",
    "    \"\"\"\n",
    "    if df_row['Gender_wiki'] != 'Unknown':\n",
    "        df_row['Gender_sso'] == df_row['Gender_wiki']\n",
    "    return df_row\n",
    "\n",
    "def impute_composer_timeline_nationality(df_row: pd.Series) -> pd.Series:\n",
    "    \"\"\"Update missing SSO/Wikipedia composer data rows with the corresponding SSO/Wikidata rows.\n",
    "\n",
    "    :param df_row: Row from merged SSO/Wikipedia composer data\n",
    "    \"\"\"\n",
    "    # only update composers that weren't in Wikipedia\n",
    "    if df_row['_merge'] == 'left_only':\n",
    "        df_row['Timeline_sso'] = df_row['Timeline_wiki']\n",
    "        df_row['Nationality_sso'] = df_row['Nationality_wiki']\n",
    "        if pd.isna(df_row['Status']):\n",
    "            df_row['Status'] = return_animate_status(df_row['Composer'], df_row['Timeline_wiki'])\n",
    "    return df_row\n",
    "\n",
    "def return_animate_status(composer: str, timeline: str) -> str:\n",
    "    \"\"\"Given a timeline, determine whether a composer is still alive.\n",
    "\n",
    "    :param composer: Full composer name\n",
    "    :param timeline: String with composer dates of birth and/or death if known\n",
    "    \"\"\"\n",
    "    # everyone is 'Dead' by default and is updated to 'Living' if there is no recorded date of death\n",
    "    # (this is unfortunately efficient due to the imbalance between dead/living classical music composers)\n",
    "    status = 'Dead'\n",
    "\n",
    "    # patterns like '1970-' are more likely to indicate a living status\n",
    "    # patterns like '1970-<some string>' more often than not indicate uncertainty, so we default to 'dead' in these cases\n",
    "    if '-' in timeline:\n",
    "        timeline_split = [ item.strip() for item in timeline.split('-') ]\n",
    "        if not timeline_split[-1]:\n",
    "            status = 'Living'\n",
    "    # timeline patterns without a hyphen require some additional logic to determine status\n",
    "    else:\n",
    "        \"\"\"\n",
    "        edge case: 'b. <year>' or 'born <year>' or 'founded <year>' (for contemporary ensembles \n",
    "        that are counted as 'composers')\n",
    "        we will only plausibly update status from 'Dead' to 'Living' for people born after ~1920\n",
    "        (this range should yield a relatively minimal # of false negatives that can be manually\n",
    "        fixed later as needed)\n",
    "        \"\"\"\n",
    "        if timeline.startswith(('born', 'b.', 'founded')):\n",
    "            # update anyone born on or after 1920 as 'Living', except for a few prominent known edge cases\n",
    "            if re.search(r'(19[2-9][0-9])|(\\b2[0-9]{3})', timeline) and not re.search(r'(d[.]?|died) (\\d{4})\\Z', timeline, flags=re.IGNORECASE):\n",
    "                if composer not in ['Dominick Argento', 'David Baker', 'Dave Bartholomew', 'Cy Coleman', 'Allen Toussaint']:\n",
    "                    status = 'Living'\n",
    "        # assume status is 'Living' if the only timeline value is 19xx or 20xx\n",
    "        elif re.match(r'(19[2-9][0-9])|(\\b2[0-9]{3})', timeline):\n",
    "            status = 'Living'\n",
    "        else:\n",
    "            pass\n",
    "    # known edge case where no dates were provided in Wikipedia\n",
    "    if composer == 'Stephen Ferguson':\n",
    "        status = 'Living'\n",
    "        \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikidataUtils:\n",
    "    \"\"\"Methods for querying and parsing Wikidata responses.\"\"\"\n",
    "\n",
    "    SSO_COMPOSER_NAME_MAP_FILE = 'data\\sso_composer_name_map.csv'\n",
    "    WIKIDATA_NATIONALITY_MAP_FILE = 'data\\wikidata_country_name_map.csv'\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    @classmethod\n",
    "    def composer_name_map(cls, sso_composer_name_map_file: str = SSO_COMPOSER_NAME_MAP_FILE):\n",
    "        \"\"\"Maps SSO composer last names to their full names.\n",
    "\n",
    "        :param sso_composer_name_map_file: CSV file with composer name mappings\n",
    "        \"\"\"\n",
    "        sso_composer_name_map = pd.read_csv(sso_composer_name_map_file, usecols=['ComposerFullName', 'Gender'], encoding='utf-8').drop_duplicates().reset_index(drop=True)\n",
    "        return sso_composer_name_map.rename(columns={'ComposerFullName': 'Composer'})\n",
    "    \n",
    "    @classmethod\n",
    "    def nationality_map(cls, nationality_map_file: str = WIKIDATA_NATIONALITY_MAP_FILE) -> Dict:\n",
    "        \"\"\"Translates Wikidata country names into a more standard (and sometimes shorter) format.\n",
    "\n",
    "        :param nationality_map_file: CSV file with Wikidata nationality mappings\n",
    "        \"\"\"\n",
    "        nationality_map = file_utils.ProcessCSV().load_csv(nationality_map_file)\n",
    "        if not (('wikidataCountry' in nationality_map) and ('Country' in nationality_map)):\n",
    "            raise ValueError(\"Error: Did not find wikidataCountry or Country columns\")\n",
    "        return dict(zip(nationality_map['wikidataCountry'], nationality_map['Country']))\n",
    "    \n",
    "    @staticmethod\n",
    "    def query_composer_nationalities_and_dates(cleaned_df: pd.DataFrame, wikipedia_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Query Wikidata API for details on SSO composer nationality, dates of birth and/or death, and gender.\n",
    "        \n",
    "        :param sso_cleaned_df: DataFrame with cleaned SSO data\n",
    "        :param wikipedia_df: DataFrame with cleaned Wikipedia composer data\n",
    "        \"\"\"\n",
    "        # we only care about composers that couldn't be matched with Wikipedia records\n",
    "        merged_df = pd.DataFrame(deduplicate_sso_composers(cleaned_df['Composer']), columns=['Composer']).merge(wikipedia_df, how='left', left_on='Composer', right_on='Composer', indicator=True)[['Composer', 'Timeline', 'Nationality', 'Status', '_merge']]\n",
    "        sso_composers = list(merged_df.loc[merged_df['_merge'] == 'left_only', 'Composer'])\n",
    "        composer_sparql_query = []\n",
    "        composer_sparql_query.append(\"\"\"\n",
    "        SELECT DISTINCT ?composer ?composer_name ?date_of_birth ?date_of_death ?nationality ?gender WHERE {\n",
    "            { ?composer (wdt:P106/(wdt:P279*)) wd:Q36834. }\n",
    "            UNION\n",
    "            { ?composer (wdt:P106/(wdt:P279*)) wd:Q15981151. }\n",
    "            UNION\n",
    "            { ?composer (wdt:P106/(wdt:P279*)) wd:Q1278335. }\n",
    "            ?composer rdfs:label ?composer_name.\n",
    "            OPTIONAL { ?composer wdt:P569 ?date_of_birth. }\n",
    "            OPTIONAL { ?composer wdt:P570 ?date_of_death. }\n",
    "            OPTIONAL { ?composer wdt:P27 ?country_of_citizenship. }\n",
    "            OPTIONAL { ?composer wdt:P21 ?sex_or_gender. }\n",
    "            FILTER((LANG(?composer_name)) = \"en\")\n",
    "            FILTER(STR(?composer_name) IN (\"\"\")\n",
    "        composer_sparql_query.append(', '.join([ f\"\\\"{composer}\\\"\" for composer in sso_composers if (composer != 'Unknown') and (' and ' not in composer) ]))\n",
    "        composer_sparql_query.append(\"\"\"))\n",
    "            SERVICE wikibase:label {\n",
    "                bd:serviceParam wikibase:language \"en\".\n",
    "                ?country_of_citizenship rdfs:label ?nationality.\n",
    "                ?sex_or_gender rdfs:label ?gender.\n",
    "            }\n",
    "        }\n",
    "        ORDER BY (?composer_name)\n",
    "        \"\"\")\n",
    "        response = return_sparql_query_results(''.join(composer_sparql_query))\n",
    "        return response\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_composer_nationalities_and_dates(nationality_map: dict, wikidata_resp: dict) -> Dict[List[str], List[str]]:\n",
    "        \"\"\"Extract composer nationality, dates of birth and/or date, and gender from Wikidata SPARQL query response.\n",
    "        \n",
    "        :param nationality_map: Wikidata country name map, as generated by nationality_map()\n",
    "        :param wikidata_resp: Wikidata SPARQL query response, as generated by query_composer_nationalities_and_dates()\n",
    "        \"\"\"\n",
    "        wikidata_composers = {'Composer': [], 'Timeline': [],'Nationality': [], 'Gender': []}\n",
    "        for row in wikidata_resp['results']['bindings']:\n",
    "            composer = row['composer_name']['value']\n",
    "            # if there are multiple records for one conductor, only use the first record\n",
    "            if composer not in wikidata_composers['Composer']:\n",
    "                nationality = 'Unknown'\n",
    "                gender = 'Unknown'\n",
    "                wikidata_composers['Composer'].append(composer)\n",
    "                # set nationality\n",
    "                if 'nationality' in row:\n",
    "                    # edge case: impute correct nationality for conflicting records\n",
    "                    if composer == 'Erwin Schulhoff':\n",
    "                        nationality = 'Czech Republic'\n",
    "                    # else use nationality value from wikidata response\n",
    "                    else:\n",
    "                        nationality = nationality_map[row['nationality']['value']]\n",
    "                # set gender\n",
    "                if 'gender' in row:\n",
    "                    gender = row['gender']['value']\n",
    "                wikidata_composers['Nationality'].append(nationality)\n",
    "                wikidata_composers['Gender'].append(gender.title())\n",
    "                # set timeline\n",
    "                date_of_birth = datetime.strptime(row['date_of_birth']['value'], '%Y-%m-%dT%H:%M:%SZ').strftime('%Y')\n",
    "                if 'date_of_death' in row:\n",
    "                    date_of_death = datetime.strptime(row['date_of_death']['value'], '%Y-%m-%dT%H:%M:%SZ').strftime('%Y')\n",
    "                    timeline = f\"{date_of_birth}-{date_of_death}\"\n",
    "                else:\n",
    "                    timeline = f\"born {date_of_birth}\"\n",
    "                wikidata_composers['Timeline'].append(timeline)\n",
    "        return wikidata_composers\n",
    "    \n",
    "    @classmethod\n",
    "    def sso_wikipedia_composers_combined(cls, cleaned_df: pd.DataFrame, wikipedia_df: pd.DataFrame):\n",
    "        \"\"\"Merge SSO and Wikipedia composer data.\n",
    "        \n",
    "        :param cleaned_df: DataFrame with cleaned SSO data\n",
    "        :param wikipedia_df: DataFrame with cleaned Wikipedia composer data. Required in order to generate delta composer list.\n",
    "        \"\"\"\n",
    "        sso_wikipedia_merged_df = pd.DataFrame(deduplicate_sso_composers(cleaned_df['Composer']), columns=['Composer']).merge(wikipedia_df, how='left', left_on='Composer', right_on='Composer', indicator=True)[['Composer', 'Timeline', 'Nationality', 'Status', '_merge']]\n",
    "        return sso_wikipedia_merged_df\n",
    "    \n",
    "    @classmethod\n",
    "    def merge_wikidata_and_sso_composers(cls, cleaned_df: pd.DataFrame, wikipedia_df: pd.DataFrame, export: bool = 0) -> Dict:\n",
    "        \"\"\"Merge Wikidata and SSO composer data.\n",
    "        \n",
    "        :param cleaned_df: DataFrame with cleaned SSO data\n",
    "        :param wikipedia_df: DataFrame with cleaned Wikipedia composer data. Required in order to generate delta composer list.\n",
    "        :param export: Marks whether to export the merged data to CSV [1 = yes, 0 = no]\n",
    "        \"\"\"\n",
    "        wikidata_nationality_map = cls.nationality_map()\n",
    "        wikidata_sparql_resp = cls.query_composer_nationalities_and_dates(cleaned_df)\n",
    "        # we only care about composers that couldn't be matched with existing Wikipedia records\n",
    "        sso_wikipedia = cls.sso_wikipedia_composers_combined(cleaned_df, wikipedia_df)\n",
    "        sso_composers = sso_wikipedia.loc[sso_wikipedia['_merge'] == 'left_only'][['Composer']]\n",
    "        # extract Wikidata details for matched composers\n",
    "        wikidata_composers = pd.DataFrame(cls.parse_composer_nationalities_and_dates(wikidata_nationality_map, wikidata_sparql_resp))\n",
    "\n",
    "        # merge delta SSO composer records with Wikidata composer records. Fill missing values with 'Unknown'.\n",
    "        wikidata_merged_df = sso_composers.merge(wikidata_composers, how='left', left_on='Composer', right_on='Composer').fillna('Unknown').drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        # manually impute correct values for the known 'Unknown's\n",
    "        nationality_missing_dict = {\n",
    "            'Australia': ['Bryony Marks', 'Connor D’Netto', 'Ella Macens', 'Eskimo Joe', 'Harry Sdraulig', 'James Ledger', 'Katy Abbott', 'Lachlan Skipworth', 'Lee and Cleworth', 'Lisa Illean', 'Paul Stanhope', 'Peggy Polias', 'Timothy Constable'],\n",
    "            'Canada': ['An-lun Huang'],\n",
    "            'China': ['Liu Tieshan and Mao Yuan'],\n",
    "            'Finland': ['Peter Grans'],\n",
    "            'Germany': ['Detlef Reikow'],\n",
    "            'Italy': ['Alessio Murgia'],\n",
    "            'New Zealand': ['Ben Hoadley', 'Miriama Young'],\n",
    "            'United States': ['Benj Pasek and Justin Paul', 'Evanescence', 'Kaoru Watanabe', 'Robert B. and Richard M. Sherman', 'Nathaniel Stookey']\n",
    "        }\n",
    "        timeline_missing_dict = {\n",
    "            'Alessio Murgia': 'born 1965',\n",
    "            'An-lun Huang': 'born 1949',\n",
    "            'Ben Hoadley': 'born 197?', # status: alive\n",
    "            'Benj Pasek and Justin Paul': 'born 1985',\n",
    "            'Bryony Marks': 'born 1971',\n",
    "            'Connor D’Netto': 'born 1994',\n",
    "            'Detlef Reikow': 'born 19??', # status: alive\n",
    "            'Ella Macens': 'born 1991',\n",
    "            'Eskimo Joe': 'born 1997',\n",
    "            'Evanescence': 'born 1995',\n",
    "            'Harry Sdraulig': 'born 1992',\n",
    "            'Kaoru Watanabe': 'born 1976',\n",
    "            'Lachlan Skipworth': 'born 1982',\n",
    "            'Lee and Cleworth': 'born 19??',\n",
    "            'Liu Tieshan and Mao Yuan': 'born 192?',\n",
    "            'Lisa Illean': 'born 1983',\n",
    "            'Miriama Young': 'born 1975',\n",
    "            'Paul Stanhope': 'born 1969',\n",
    "            'Peggy Polias': 'born 1981',\n",
    "            'Peter Grans': 'born 1954',\n",
    "            'Robert B. and Richard M. Sherman': 'born 1925/8',\n",
    "            'Nathaniel Stookey': 'born 1970',\n",
    "            'Timothy Constable': 'born 1981'\n",
    "        }\n",
    "        # update existing gender with whatever is in Wikidata\n",
    "        gender_missing_dict = {\n",
    "            'Female': ['Bryony Marks', 'Ella Macens', 'Lisa Illean', 'Miriama Young', 'Peggy Polias'],\n",
    "            'Male': ['Alessio Murgia', 'An-lun Huang', 'Ben Hoadley', 'Benj Pasek and Justin Paul', 'Connor D’Netto', 'Detlef Reikow', 'Harry Sdraulig', 'Kaoru Watanabe', 'Lachlan Skipworth', 'Lee and Cleworth', 'Liu Tieshan and Mao Yuan', 'Paul Stanhope', 'Peter Grans', 'Robert B. and Richard M. Sherman', 'Nathaniel Stookey', 'Timothy Constable']\n",
    "        }\n",
    "        for country in nationality_missing_dict:\n",
    "            for composer in nationality_missing_dict[country]:\n",
    "                if wikidata_merged_df.loc[wikidata_merged_df['Composer'] == composer, 'Nationality'].values == 'Unknown':\n",
    "                    wikidata_merged_df.loc[wikidata_merged_df['Composer'] == composer, 'Nationality'] = country\n",
    "        for composer in timeline_missing_dict:\n",
    "            if wikidata_merged_df.loc[wikidata_merged_df['Composer'] == composer, 'Timeline'].values == 'Unknown':\n",
    "                wikidata_merged_df.loc[wikidata_merged_df['Composer'] == composer, 'Timeline'] = timeline_missing_dict[composer]\n",
    "        for gender in gender_missing_dict:\n",
    "            for composer in gender_missing_dict[gender]:\n",
    "                if wikidata_merged_df.loc[wikidata_merged_df['Composer'] == composer, 'Gender'].values == 'Unknown':\n",
    "                    wikidata_merged_df.loc[wikidata_merged_df['Composer'] == composer, 'Gender'] = gender\n",
    "        \n",
    "        # merge all the things\n",
    "        sso_wiki_composer_map = sso_wikipedia.merge(cls.composer_name_map(), how='right', left_on='Composer', right_on='Composer')\n",
    "        final_merged_df = sso_wiki_composer_map.merge(wikidata_merged_df, how='left', left_on='Composer', right_on='Composer', suffixes=('_sso', '_wiki'))\n",
    "        final_merged_df = final_merged_df.apply(lambda row: impute_composer_gender(row), axis=1)\n",
    "        final_merged_df = final_merged_df.apply(lambda row: impute_composer_timeline_nationality(row), axis=1)\n",
    "        final_merged_df = final_merged_df.drop(columns=['_merge', 'Timeline_wiki', 'Nationality_wiki', 'Gender_wiki']).rename(columns={'Timeline_sso': 'Timeline', 'Nationality_sso': 'Nationality', 'Gender_sso': 'Gender'}).fillna('Unknown')\n",
    "\n",
    "        # final ad-hoc fixes\n",
    "        composers_to_fix = ['Ben Hoadley', 'Liu Tieshan and Mao Yuan', 'Detlef Reikow']\n",
    "        for composer in composers_to_fix:\n",
    "            final_merged_df.loc[final_merged_df['Composer'] == composer, 'Status'] = 'Living'\n",
    "\n",
    "        # write to CSV if export = 1\n",
    "        if export == 1:\n",
    "            out_file = 'sso_composer_nationality_map.csv'\n",
    "            final_merged_df.to_csv(path_or_buf=out_file, encoding='utf-8', index=False)\n",
    "            print(f\"Wrote composer nationality map to: {out_file}\")\n",
    "        return final_merged_df\n",
    "\n",
    "    @staticmethod\n",
    "    def query_conductor_nationalities(cleaned_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Query Wikidata API for details on SSO conductor nationality and gender.\n",
    "        \n",
    "        :param sso_cleaned_df: DataFrame with cleaned SSO data\n",
    "        \"\"\"\n",
    "        conductor_sparql_query = []\n",
    "        conductor_sparql_query.append(\"\"\"\n",
    "        SELECT DISTINCT ?conductor ?conductor_name ?nationality ?gender WHERE {\n",
    "            ?conductor (wdt:P106/(wdt:P279*)) wd:Q1198887;\n",
    "                                rdfs:label ?conductor_name.\n",
    "            OPTIONAL { ?conductor wdt:P27 ?country_of_citizenship. }\n",
    "            OPTIONAL { ?conductor wdt:P21 ?sex_or_gender. }\n",
    "            FILTER(LANG(?conductor_name) = \"en\").\n",
    "            FILTER(STR(?conductor_name) IN (\"\"\")\n",
    "        conductor_sparql_query.append(', '.join([ f\"\\\"{conductor}\\\"\" for conductor in sorted(set(cleaned_df['Conductor'])) if conductor != 'Unknown']))\n",
    "        conductor_sparql_query.append(\"\"\"))\n",
    "            SERVICE wikibase:label {\n",
    "                bd:serviceParam wikibase:language \"en\".\n",
    "                ?country_of_citizenship rdfs:label ?nationality.\n",
    "                ?sex_or_gender rdfs:label ?gender.\n",
    "            }\n",
    "        }\n",
    "        ORDER BY (?conductor_name)\n",
    "        \"\"\")\n",
    "        response = return_sparql_query_results(''.join(conductor_sparql_query))\n",
    "        return response\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_conductor_nationalities(nationality_map: dict, wikidata_resp: dict) -> Dict[List[str], List[str]]:\n",
    "        \"\"\"Extract conductor nationality and gender from Wikidata SPARQL query response.\n",
    "        \n",
    "        :param nationality_map: Wikidata country name map, as generated by nationality_map()\n",
    "        :param wikidata_resp: Wikidata SPARQL query response, as generated by query_conductor_nationalities()\n",
    "        \"\"\"\n",
    "        wikidata_conductors = {'Conductor': [], 'Nationality': [], 'Gender': []}\n",
    "        for row in wikidata_resp['results']['bindings']:\n",
    "            conductor = row['conductor_name']['value']\n",
    "            # if there are multiple records for one conductor, only use the first record\n",
    "            if conductor not in wikidata_conductors['Conductor']:\n",
    "                nationality = 'Unknown'\n",
    "                gender = 'Unknown'\n",
    "                wikidata_conductors['Conductor'].append(conductor)\n",
    "                # set nationality\n",
    "                if 'nationality' in row:\n",
    "                    # edge case: correct erroneous nationality ('English')\n",
    "                    if conductor == 'Marin Alsop':\n",
    "                        nationality = 'United States'\n",
    "                    # else use nationality value from wikidata response\n",
    "                    else:\n",
    "                        nationality = nationality_map[row['nationality']['value']]\n",
    "                # set gender\n",
    "                if 'gender' in row:\n",
    "                    gender = row['gender']['value']\n",
    "                wikidata_conductors['Nationality'].append(nationality)\n",
    "                wikidata_conductors['Gender'].append(gender.title())\n",
    "        return wikidata_conductors\n",
    "\n",
    "    @classmethod\n",
    "    def merge_wikidata_and_sso_conductors(cls, cleaned_df: pd.DataFrame, export: bool = 0) -> Dict:\n",
    "        \"\"\"Merge Wikidata and SSO conductor data, and manually impute missing values.\n",
    "        \n",
    "        :param cleaned_df: DataFrame with cleaned SSO data\n",
    "        :param export: Marks whether to export the merged data to CSV [1 = yes, 0 = no]\n",
    "        \"\"\"\n",
    "        wikidata_nationality_map = cls.nationality_map()\n",
    "        wikidata_sparql_resp = cls.query_conductor_nationalities(cleaned_df)\n",
    "        sso_conductors = pd.DataFrame(sorted(set(cleaned_df['Conductor'])), columns=['Conductor'])\n",
    "        wikidata_conductors = pd.DataFrame(cls.parse_conductor_nationalities(wikidata_nationality_map, wikidata_sparql_resp))\n",
    "\n",
    "        # merge SSO conductor records with Wikidata conductor records. Fill missing values with 'Unknown'.\n",
    "        merged_df = sso_conductors.merge(wikidata_conductors, how='left', left_on='Conductor', right_on='Conductor').fillna('Unknown')\n",
    "        # manually impute correct values for the known 'Unknown's\n",
    "        nationality_missing_dict = {\n",
    "            'Australia': ['Benjamin Northey', 'Brett Kelly', 'Brett Weymark', 'Dane Lam', 'Fabian Russell', 'Guy Noble', 'Iain Grandage', 'Nicholas Buc', 'Nicholas Carter', 'Vanessa Scammell'],\n",
    "            'Italy': ['Umberto Clerici'],\n",
    "            'New Zealand': ['Hamish McKeich'],\n",
    "            'Singapore': ['Joshua Tan'],\n",
    "            'United Kingdom': ['Andrew Haveron', 'Finnegan Downie Dear', 'Roger Benedict'],\n",
    "            'United States': ['Erik Ochsner', 'Marc Taddei']\n",
    "        }\n",
    "        gender_missing_dict = {\n",
    "            'Female': ['Vanessa Scammell'],\n",
    "            'Male': ['Andrew Haveron', 'Brett Kelly', 'Brett Weymark', 'Dane Lam', 'Erik Ochsner', 'Fabian Russell', 'Finnegan Downie Dear', 'Guy Noble', 'Hamish McKeich', 'Iain Grandage', 'Joshua Tan', 'Nicholas Buc', 'Nicholas Carter', 'Roger Benedict', 'Umberto Clerici']\n",
    "        }\n",
    "        for country in nationality_missing_dict:\n",
    "            for conductor in nationality_missing_dict[country]:\n",
    "                if merged_df.loc[merged_df['Conductor'] == conductor, 'Nationality'].values == 'Unknown':\n",
    "                    merged_df.loc[merged_df['Conductor'] == conductor, 'Nationality'] = country\n",
    "        for gender in gender_missing_dict:\n",
    "            for conductor in gender_missing_dict[gender]:\n",
    "                if merged_df.loc[merged_df['Conductor'] == conductor, 'Gender'].values == 'Unknown':\n",
    "                    merged_df.loc[merged_df['Conductor'] == conductor, 'Gender'] = gender\n",
    "\n",
    "        # write to CSV if export = 1\n",
    "        if export == 1:\n",
    "            out_file = 'sso_conductor_nationality_map.csv'\n",
    "            merged_df.to_csv(path_or_buf=out_file, encoding='utf-8', index=False)\n",
    "            print(f\"Wrote conductor nationality map to: {out_file}\")\n",
    "        return merged_df"
   ]
  },
  {
   "source": [
    "## Wikipedia and related utility methods\n",
    "\n",
    "Methods for querying and cleaning composer metadata from Wikipedia."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wikinode_string(node: mwparser.nodes.Node) -> str:\n",
    "    \"\"\"Extract text from a given Wikicode token.\n",
    "    \n",
    "    :param node: Accepted Wikicode types: [Template|Text|Wikilink]\n",
    "    \"\"\"\n",
    "    extracted_str = ''\n",
    "    if isinstance(node, mwparser.nodes.template.Template):\n",
    "        if node.params and not node.has('colwidth'):\n",
    "            extracted_str = node.params[0]\n",
    "        else:\n",
    "            if node.name in ['spaced ndash', 'snd']:\n",
    "                extracted_str = '---DELETE---'\n",
    "    elif isinstance(node, mwparser.nodes.wikilink.Wikilink):\n",
    "        extracted_str = node.title\n",
    "    else:\n",
    "        if ' – ' in node.value:\n",
    "            # in strings like ' (1900 – 1975) – some extraneous text', replace second hyphen with a deletion marker\n",
    "            if re.match(r' [(](\\w.+)([ ]?[–-][ ]?)(\\w.+)[)]( [–] \\w.+)', node.value):\n",
    "                extracted_str = node.value.replace(' – ', '---DELETE---')\n",
    "            # but don't replace the hyphen if the string is a legitimate date like '(1900 – 1975)'\n",
    "            # with no trailing extraneous text\n",
    "            else:\n",
    "                extracted_str = node.value.replace(' – ', '-')\n",
    "        else:\n",
    "            extracted_str = node.value\n",
    "    return str(extracted_str)\n",
    "\n",
    "def parse_composer_wiki_xml(country_name: str, country_xml_content: mwparser.wikicode.Wikicode) -> List[str]:\n",
    "    \"\"\"Parse Wikipedia composer-by-nationality XML into a list of individual composer name items.\n",
    "\n",
    "    :param country_name: Country name\n",
    "    :param country_xml_content: Wiki XML blob for the specified country\n",
    "    \"\"\"\n",
    "    str_clean_pat = re.compile(r'[ ]?\\n[ ]?')\n",
    "    composers = []\n",
    "    if ('columns-list' in country_xml_content):\n",
    "        template_filter_list = reduce(lambda x, y: x+y, [ t.params for t in filter(lambda r: r.name == 'columns-list', country_xml_content.filter_templates()) ])\n",
    "        for composer_list in filter(lambda p: p and not p.startswith(('List', 'colwidth')), template_filter_list):\n",
    "            composer_list_cleaned = re.sub(str_clean_pat, r'|', mwparser.parse(composer_list.replace('{{ndash}}', '-')).strip_code().strip()).split('|')\n",
    "            composers.extend(composer_list_cleaned)\n",
    "    else:\n",
    "        # filter out extraneous header, tag, <ref></ref>, [[File:...]] and other content (albeit in an ugly way)\n",
    "        node_list = [ node for node in filter(lambda n: (n not in [', '] and n) and isinstance(n, (mwparser.nodes.template.Template, mwparser.nodes.text.Text, mwparser.nodes.wikilink.Wikilink)), country_xml_content.nodes) if not any(match in node for match in ['<ref', '[[File:', 'Talk:', 'INSERT A PERSON INTO A SECTION', 'List of', 'Chronological list of', 'Only add names here']) ]\n",
    "\n",
    "        # extract text from each node and then rejoin everything into a long string\n",
    "        composer_list_cleaned = mwparser.parse(''.join(list(map(extract_wikinode_string, node_list))))\n",
    "        # replace newlines with '|' and then split on '|' to generate a list of composer strings\n",
    "        # (this works because each unique composer string is separated by '\\n')\n",
    "        composer_list_cleaned = re.sub(str_clean_pat, r'|', composer_list_cleaned.strip_code().strip()).split('|')\n",
    "        # filter out empty strings\n",
    "        composer_list_cleaned = filter(lambda c: c, composer_list_cleaned)\n",
    "\n",
    "        # fix edge case with hyphen in unexpected place: 'Georg Friedrich Haas ---DELETE--- (b. 1953) Composer of contemporary classical music'\n",
    "        composers.extend([ composer.replace(' ---DELETE--- ', ' ') if composer.startswith('Georg Friedrich Haas') else composer for composer in list(composer_list_cleaned) ])\n",
    "    return composers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_composer_string(composer_string: str) -> Dict[str, str]:\n",
    "    \"\"\"Reformat composer strings into {Composer: Name, Timeline: YYYY-YYYY} format.\n",
    "    \n",
    "    :param composer_string: Most common input format: 'Aram Khachaturian (1903-1978)'\n",
    "\n",
    "    Correctly formats and splits *most* but not all composer strings. Generates a small number of \n",
    "    false negatives that can be manually fixed later.\n",
    "\n",
    "    Warning: Here lies lots of ugly regex...\n",
    "    \"\"\"\n",
    "    composer = ''\n",
    "    timeline = 'Unknown'\n",
    "\n",
    "    # 1. do some initial string normalisation\n",
    "    if isinstance(composer_string, element.Tag):\n",
    "        composer_cleaned = unicodedata.normalize('NFC', composer_string.text.replace('\\xa0', ' ').strip())\n",
    "    else:\n",
    "        composer_cleaned = unicodedata.normalize('NFC', composer_string.replace('\\xa0', ' ').strip())\n",
    "    # remove extra spaces\n",
    "    composer_cleaned = re.sub(r'[ ]{2,}', r' ', composer_cleaned)\n",
    "    \n",
    "    # 2. attempt to split original composer string into 'Composer Name|1900-1975' format\n",
    "    # edge case (with no parentheses): 'Composer Name 1900-1980'\n",
    "    if '(' not in composer_cleaned:\n",
    "        composer_cleaned = re.sub(r'^(\\w.+) (\\d.+)', r'\\1|\\2', composer_cleaned)\n",
    "    # assume everything else follows 'Composer Name (1900-1980)' or 'Composer Name (1900-1980' format\n",
    "    else:\n",
    "        # convert '(the younger)' to 'the younger'\n",
    "        composer_cleaned = re.sub(r'[(](the younger)[)]', r'\\1', composer_cleaned)\n",
    "        # filter out any text after a '---DELETE---' marker\n",
    "        if '---DELETE---' in composer_cleaned:\n",
    "            composer_cleaned = re.sub(r'(\\w.+)(---DELETE---.*)', r'\\1', composer_cleaned)\n",
    "        # filter out any content between parentheses that doesn't contain numbers\n",
    "        # e.g. we care about '(born 1900)' and '(1900-1970)', but not '(extraneous text)'\n",
    "        composer_cleaned = re.sub(r'(\\w.+)?([ ]{1,})([(]([^\\d]+)\\1[)])', r'\\1', composer_cleaned)\n",
    "\n",
    "        if ')' not in composer_string:\n",
    "            composer_cleaned = re.sub(r'^(\\w.+)[ ]?([(])(\\w.+|\\W.+)\\Z', r'\\1|\\3', composer_cleaned)\n",
    "        # edge case: 'Ernő Dohnányi (1877–1960), <lots of extraneous text>'\n",
    "        elif composer_string.startswith('Ernő Dohnányi (1877–1960)'):\n",
    "            composer_cleaned = re.sub(r'^(\\w.+) ([(])(\\w.+)([)])', r'\\1|\\3', composer_string.split(',')[0])\n",
    "        else:\n",
    "            composer_cleaned = re.sub(r'^(\\w.+)[ ]?([(])(\\w.+|\\W.+)([)])([ ,]|\\w|\\Z)([ ]?\\w.+)?', r'\\1|\\3', composer_cleaned)\n",
    "    \n",
    "    # edge case: remove any labels like '[de]'\n",
    "    composer_cleaned = re.sub(r'^(\\w.+) (\\[\\w.+\\])?([|]\\w.+)\\Z', r'\\1\\3', composer_cleaned).rstrip(')')\n",
    "    # filter edge case: 'May Brahe|née Mary Dixon 1884–1956'\n",
    "    composer_cleaned = re.sub(r'[|](née \\w.+) (\\d.+)', r'|\\2', composer_cleaned)\n",
    "\n",
    "    # 3. update composer and timeline variables\n",
    "    composer = composer_cleaned.split('|')[0]\n",
    "    if len(composer_cleaned.split('|')) > 1:\n",
    "        timeline = unidecode(composer_cleaned.split('|')[1])\n",
    "\n",
    "    # 4. clean up some additional edge cases\n",
    "    \"\"\"\n",
    "    Edge cases:\n",
    "    1. convert e.g. 'William Cornysh the younger' to 'William Cornysh II'\n",
    "    2. truncate e.g. 'Thomas Linley the elder' to 'Thomas Linley'\n",
    "    3. truncate e.g. 'Leopold I, Holy Roman Emperor' to just 'Leopold I'\n",
    "    4. truncate extraneous text after semi-colon in e.g. 'John Hanboys (14th century); may be J. de Alto Bosco'\n",
    "    \"\"\"\n",
    "    suffix_dict = {'holy roman emperor': None, 'the elder': None, 'the younger': 'II'}\n",
    "    composer = re.sub(r'(holy roman emperor)|(the (elder|younger))\\Z', lambda suffix: suffix_dict.get(suffix.group()), composer.strip('()'), flags=re.IGNORECASE).strip(', ')\n",
    "    composer = re.sub(r'(\\w.+)([ ])?([;])( \\w.+)', r'\\1', composer)\n",
    "\n",
    "    # 5. add separated composer names and birth/death year details to a dictionary\n",
    "    # if year is missing, Timeline defaults to 'Unknown'\n",
    "    return {'Composer': composer, 'Timeline': timeline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_composer_name(composer: str) -> NamedTuple:\n",
    "    \"\"\"Split composer names into parts (last name, given names, initials).\n",
    "\n",
    "    :param composer: Full composer name\n",
    "    \"\"\"\n",
    "    # extract given and last names + any suffixes\n",
    "    converted_name = HumanName(composer)\n",
    "    # if composer has just one name, move that value to the 'last' name attribute\n",
    "    if not converted_name.last:\n",
    "        converted_name.last = converted_name.first\n",
    "        converted_name.first = None\n",
    "    \n",
    "    # limit list of middle names to just 2 items since we only need 2 initials max\n",
    "    if len(converted_name.middle_list) > 2:\n",
    "        middle_names_truncated = converted_name.middle_list[:2]\n",
    "    else:\n",
    "        middle_names_truncated = converted_name.middle_list\n",
    "\n",
    "    # extract first, second and/or third initials from name\n",
    "    converted_initials = {'Initial1': None, 'Initial2': None, 'Initial3': None}\n",
    "    if converted_name.middle_list:\n",
    "        converted_initials['Initial1'] = converted_name.first[0]\n",
    "        converted_initials['Initial2'] = middle_names_truncated[0][0]\n",
    "        if len(middle_names_truncated) > 1:\n",
    "            converted_initials['Initial3'] = middle_names_truncated[1][0]\n",
    "    else:\n",
    "        if not converted_name.first:\n",
    "            converted_initials['Initial1'] = converted_name.last[0]\n",
    "        else:\n",
    "            converted_initials['Initial1'] = converted_name.first[0]\n",
    "\n",
    "    # return split name + any initials\n",
    "    SplitName = namedtuple('SplitName', 'LastName GivenNames Initial1 Initial2 Initial3')\n",
    "    if converted_name.suffix:\n",
    "        return SplitName(LastName=f\"{converted_name.last} {converted_name.suffix}\".strip(), GivenNames=f\"{converted_name.first} {converted_name.middle}\".strip(), Initial1=converted_initials['Initial1'], Initial2=converted_initials['Initial2'], Initial3=converted_initials['Initial3'])\n",
    "    else:\n",
    "        return SplitName(LastName=converted_name.last.strip(), GivenNames=f\"{converted_name.first} {converted_name.middle}\".strip(), Initial1=converted_initials['Initial1'], Initial2=converted_initials['Initial2'], Initial3=converted_initials['Initial3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## parse Wikipedia omnibus Composers nationality lists\n",
    "def parse_composer_nationalities(master_xml: str, subsection_xml: str, mastersections: bool = 1, subsections: bool = 0) -> pd.DataFrame:\n",
    "    \"\"\"Returns list of dicts of composers, dates of birth and/or death, nationalities, and status (living/dead).\n",
    "\n",
    "    :param master_xml: Wiki XML file with master list of composers by nationality\n",
    "    :param subsection_xml: Wiki XML file with per-country list of composers by nationality. For countries with larger numbers of composers that aren't included on the master wiki.\n",
    "    :param mastersections: Toggle parsing the master list (default = 1)\n",
    "    :param subections: Toggle parsing the per-country list (default = 0)\n",
    "    \"\"\"\n",
    "    # countries for which we want to process separate wiki sub-pages - map adjectives -> nouns\n",
    "    country_dict = {'American': 'United States', 'Armenian': 'Armenia', 'Australian': 'Australia', 'Austrian': 'Austria', 'Canadian': 'Canada', 'Czech': 'Czech Republic', 'Estonian': 'Estonia', 'French': 'France', 'German': 'Germany', 'Icelandic': 'Iceland', 'Italian': 'Italy', 'English': 'United Kingdom'}\n",
    "    subsection_countries = sorted(set(country_dict.values()))\n",
    "\n",
    "    # section headers to exclude\n",
    "    exclude_sections = ['Contents', 'External links', 'Further reading', 'References', 'See also']\n",
    "    # regex exclude pattern needs to be a literal string since mwparser.get_sections() doesn't accept re.compile\n",
    "    exclude_pattern = \"r\\'^(?!(\" + '([ ]{1,})?(' + '|'.join(exclude_sections) + ')([ ]{1,})?' + \")).*\\'\"\n",
    "    \n",
    "    # master list for combining parsed data from the master wiki and sub-wikis\n",
    "    master_composer_list = []\n",
    "\n",
    "    # 1. loop through all countries from the master wiki and extract composer details\n",
    "    if mastersections == 1:\n",
    "        master_wiki_sections = mwparser.parse(re.sub(r'<!--|-->', '', file_utils.ProcessWikiXML().load_xml(master_xml).find('text', attrs={'xml:space': 'preserve'}).text))\n",
    "        for master_country in master_wiki_sections.get_sections(matches=eval(exclude_pattern)):\n",
    "            master_country_name = master_country.filter_headings()[0].title.strip()\n",
    "            master_wiki_list = []\n",
    "            \"\"\"\n",
    "            skip countries with wiki sub-pages that we want to process separately\n",
    "            we exclude the 'United States' section of the master wiki in favour of the more\n",
    "            comprehensive wiki sub-page\n",
    "            \"\"\"\n",
    "            if (master_country_name not in subsection_countries) and (master_country_name not in ['Iran', 'Ireland', 'Japan', 'Lithuania', 'Mongolia', 'Philippines', 'Slovakia', 'Slovenia', 'South Africa', 'Sri Lanka']):\n",
    "                master_wiki_list.extend(parse_composer_wiki_xml(master_country_name, master_country))\n",
    "            # construct dict for each composer and add to master composers list\n",
    "            for master_composer in master_wiki_list:\n",
    "                master_composer = reformat_composer_string(master_composer)\n",
    "                master_composer['Nationality'] = master_country_name\n",
    "                master_composer['Status'] = return_animate_status(master_composer['Composer'], master_composer['Timeline'])\n",
    "                master_composer_list.append(master_composer)\n",
    "\n",
    "    # 2. loop through all countries from the subsection country wikis and extract composer details\n",
    "    # 'subsection country' = country title is on the master wiki, but its actual full content is on a separate wiki\n",
    "    if subsections == 1:\n",
    "        sub_wiki_soup = file_utils.ProcessWikiXML().load_xml(subsection_xml)\n",
    "        # generate list of tuples: (country_name, sanitised_xml_content)\n",
    "        sub_wiki_countries = list(zip([ country_dict[re.sub(r'((\\w+ )?List of )(\\w+) ((\\w+ )?composers)', r'\\3', title.text, flags=re.IGNORECASE)] for title in sub_wiki_soup.find_all('title') ], [ mwparser.parse(re.sub(r'<!--|-->', '', blob.text)) for blob in sub_wiki_soup.find_all('text', attrs={'xml:space': 'preserve'}) ]))\n",
    "\n",
    "        for sub_country_name, sub_wiki_text in sub_wiki_countries:\n",
    "            sections = sub_wiki_text.get_sections(matches=eval(exclude_pattern))\n",
    "            for section in sections:\n",
    "                section_list = []\n",
    "                section_list.extend(parse_composer_wiki_xml(sub_country_name, section))\n",
    "                # construct dict for each composer and add to master_composer_list\n",
    "                for idx, sub_composer in enumerate(section_list):\n",
    "                    sub_composer = reformat_composer_string(sub_composer)\n",
    "                    sub_composer['Nationality'] = sub_country_name\n",
    "                    sub_composer['Status'] = return_animate_status(sub_composer['Composer'], sub_composer['Timeline'])\n",
    "                    section_list[idx] = sub_composer\n",
    "                master_composer_list.extend(section_list)\n",
    "\n",
    "    # 3. return the aggregated list of composer details\n",
    "    return master_composer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_remove_nationality_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove duplicates and manually reconcile conflicting entries from parsed Wikipedia 'List of composers by nationality'.\n",
    "\n",
    "    :param df: Master DataFrame of parsed Wiki XML data\n",
    "    \"\"\"\n",
    "    fixed_df = df.copy()\n",
    "\n",
    "    # 1a. Sanitise composer strings with pattern 'Composer Name (extraneous text'\n",
    "    fixed_df['Composer'] = fixed_df['Composer'].apply(lambda name: re.sub(r' [(]([^\\d]+)\\Z', r'', name).strip() if re.search(r' [(]([^\\d]+)\\Z', name) else name)\n",
    "\n",
    "    # 1b. Try a second-pass reformat of sanitised composer strings\n",
    "    # also pre-sanitises edge case string ':fr:Jacques de la Presle (1888–1969'\n",
    "    composer_replace_dict = dict(fixed_df.loc[fixed_df['Composer'].str.contains('\\('), 'Composer'].apply(lambda name: tuple(reformat_composer_string(re.sub(r'^[:]\\w{1,2}[:]', r'', name)).values())))\n",
    "    for idx, row in fixed_df.loc[fixed_df['Composer'].str.contains('\\(')].iterrows():\n",
    "        fixed_df.loc[idx, 'Composer'] = composer_replace_dict[idx][0]\n",
    "        fixed_df.loc[idx, 'Timeline'] = composer_replace_dict[idx][1]\n",
    "        fixed_df.loc[idx, 'Status'] = return_animate_status(composer_replace_dict[idx][0], composer_replace_dict[idx][1])\n",
    "\n",
    "    # 1c. Drop full duplicates (having same Composer/Timeline/Nationality)\n",
    "    fixed_df = fixed_df.drop_duplicates().sort_values(by=['Composer', 'Timeline', 'Nationality']).reset_index(drop=True)\n",
    "\n",
    "    # 2. manually reconcile conflicting nationalities (same Composer/Timeline, but multiple Nationalities)\n",
    "    composers_to_update = {\n",
    "        'Argentina': ['Analia Llugdar'],\n",
    "        'Armenia': ['Aram Khachaturian', 'Karen Khachaturian'],\n",
    "        'Austria': ['Carl Czerny', 'Joseph Haydn', 'Johann Nepomuk Hummel', 'Gustav Mahler', 'Leopold Mozart', 'Wolfgang Amadeus Mozart', 'Arnold Schoenberg', 'Franz Schreker', 'Franz Schubert'], \n",
    "        'Australia': ['Julian Cochran'], \n",
    "        'Belgium': ['Jean-Baptiste Accolay'], \n",
    "        'Bulgaria': ['Alexandra Fol'], \n",
    "        'Canada': ['León Zuckert'], \n",
    "        'Colombia': ['Kike Santander'],\n",
    "        'Croatia': ['Elena Pucić-Sorkočević'], \n",
    "        'Czech Republic': ['Franz Benda', 'Florian Leopold Gassmann', 'Karel Husa', 'Johann Pehel', 'Johann Baptist Wanhal', 'Jaromír Weinberger'], \n",
    "        'Estonia': ['Boris Parsadanian', 'Helen Tobias-Duesberg'], \n",
    "        'France': ['Arthur Honegger', 'Jean-Baptiste Lully', 'Jacques Offenbach', 'Alexandre Tansman'], \n",
    "        'Germany': ['Christoph Willibald Gluck', 'George Frideric Handel', 'Johann Christoph Pepusch', 'Franz Xaver Richter', 'Carl Stamitz'],\n",
    "        'Hungary': ['Franz Liszt'],\n",
    "        'Netherlands': ['Fred Momotenko'], \n",
    "        'Poland': ['Krzysztof Penderecki'], \n",
    "        'Puerto Rico': ['Roberto Sierra'],\n",
    "        'Romania': ['Mihail Jora'], \n",
    "        'Russia': ['Alla Pavlova'], \n",
    "        'United Kingdom': ['Samuel Coleridge-Taylor', 'Anthony de Countie', 'Eugene Goossens'],\n",
    "        'United States': ['Leonardo Balada', 'Henry Brant', 'Douglas Knehans', 'Ernst Krenek', 'Gian Carlo Menotti', 'Conlon Nancarrow']\n",
    "        }\n",
    "    for country in composers_to_update:\n",
    "        for composer in composers_to_update[country]:\n",
    "            fixed_df.loc[fixed_df['Composer'] == composer, 'Nationality'] = country\n",
    "\n",
    "    # 3. drop ad-hoc duplicates\n",
    "    fixed_df = fixed_df.loc[~(((fixed_df['Composer'] == 'Ruby Claudia Davy') | (fixed_df['Composer'] == 'May Howlett')) & (fixed_df['Timeline'] == 'Unknown'))]\n",
    "\n",
    "    # 4. also fix erroneous timeline data for misc duplicate records\n",
    "    fixed_df.loc[fixed_df['Composer'] == 'Mona McBurney', 'Timeline'] = '1862-1932'\n",
    "    fixed_df.loc[fixed_df['Composer'] == 'Johann Christoph Pepusch', 'Timeline'] = '1667-1752'\n",
    "    fixed_df.loc[fixed_df['Composer'] == 'Giacomo Puccini', 'Timeline'] = '1858-1924'\n",
    "    fixed_df.loc[fixed_df['Composer'] == 'Carl Stamitz', 'Timeline'] = '1745-1801' \n",
    "    fixed_df.loc[fixed_df['Composer'] == 'May Summerbelle', 'Timeline'] = '1867-1947'\n",
    "\n",
    "    fixed_df = fixed_df.drop_duplicates().reset_index(drop=True)\n",
    "    return fixed_df\n",
    "\n",
    "def wiki_clean_nationality_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ad-hoc manual fixes nationality data edge cases that regex couldn't handle\n",
    "\n",
    "    :param df: De-duplicated master DataFrame of parsed Wiki XML data \n",
    "    \"\"\"\n",
    "    cleaned_df = df.copy()\n",
    "    ## drop bad record\n",
    "    cleaned_df = cleaned_df.loc[cleaned_df['Composer'] != '3']\n",
    "\n",
    "    ## standardise Russian surname transliteration\n",
    "    cleaned_df.loc[cleaned_df['Composer'] == 'Sergei Rachmaninoff', 'Composer'] = 'Sergei Rachmaninov'\n",
    "\n",
    "    ## fix record with Chinese characters\n",
    "    cleaned_df.loc[cleaned_df['Composer'].str.contains('Liu Shueh-Shuan'), 'Composer'] = 'Liu Shueh-Shuan'\n",
    "\n",
    "    ## edge cases that regex couldn't handle\n",
    "    # fix India edge cases (mostly ensembles)\n",
    "    cleaned_df.loc[cleaned_df['Composer'].str.contains('Jatin-Lalit'), 'Composer'] = 'Jatin-Lalit'\n",
    "    cleaned_df.loc[cleaned_df['Composer'].str.contains('Laxmikant–Pyarelal'), 'Composer'] = 'Laxmikant–Pyarelal'\n",
    "    cleaned_df.loc[cleaned_df['Composer'].str.contains('Nadeem-Shravan'), 'Composer'] = 'Nadeem-Shravan'\n",
    "    cleaned_df.loc[cleaned_df['Composer'].str.contains('Shankar–Ehsaan–Loy'), 'Composer'] = 'Shankar–Ehsaan–Loy'\n",
    "    # fix New Zealand edge case\n",
    "    cleaned_df.loc[cleaned_df['Composer'].str.contains('Wayan Yudane'), 'Composer'] = 'Wayan Yudane'\n",
    "\n",
    "    cleaned_df = cleaned_df.drop_duplicates().sort_values(by=['Composer', 'Timeline']).reset_index(drop=True)\n",
    "    return cleaned_df"
   ]
  },
  {
   "source": [
    "## Merge SSO data with generated composer and conductor metadata"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_conductor_composer_metadata(cleaned_df: pd.DataFrame, conductor_map_file: str, composer_map_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Merge the cleaned SSO master data set with pre-aggregated composer and conductor metadata.\n",
    "\n",
    "    :param cleaned_df: DataFrame with cleaned SSO data\n",
    "    :param conductor_map_file: Name of CSV with SSO conductor nationality/gender mappings\n",
    "    :param composer_map_file: Name of CSV with SSO composer nationality/status/gender mappings\n",
    "    \"\"\"\n",
    "    # work with a modified copy of the original DataFrame\n",
    "    sso_df_copy = cleaned_df.copy()\n",
    "    # temporarily convert Artist_Metadata into a string representation so that it can be added to a multiindex\n",
    "    sso_df_copy['Artist_Metadata'] = sso_df_copy['Artist_Metadata'].apply(lambda x: str(x))\n",
    "\n",
    "    # load previously generated conductor and composer metadata maps\n",
    "    conductor_nationality_map = file_utils.ProcessCSV().load_csv(conductor_map_file)\n",
    "    composer_nationality_map = file_utils.ProcessCSV().load_csv(composer_map_file)\n",
    "\n",
    "    # add conductor metadata\n",
    "    sso_df_copy = pd.concat([sso_df_copy.drop('Conductor', axis=1), sso_df_copy[['Conductor']].merge(conductor_nationality_map, how='left', left_on='Conductor', right_on='Conductor').rename(columns={'Nationality': 'ConductorNationality', 'Gender': 'ConductorGender'})], axis=1)\n",
    "\n",
    "    # explode Piece and Composer lists into separate rows\n",
    "    sso_df_copy = sso_df_copy.set_index(['Concert', 'Key', 'Date', 'Conductor', 'ConductorNationality', 'ConductorGender', 'Artist_Metadata'])[['Piece', 'Composer']].apply(lambda x: pd.Series.explode(x)).reset_index()\n",
    "    # add composer metadata\n",
    "    sso_df_copy = pd.concat([sso_df_copy.drop('Composer', axis=1), sso_df_copy[['Composer']].merge(composer_nationality_map, how='left', left_on='Composer', right_on='Composer').rename(columns={'Nationality': 'ComposerNationality', 'Timeline': 'ComposerTimeline', 'Status': 'ComposerStatus', 'Gender': 'ComposerGender'})], axis=1)\n",
    "\n",
    "    # generate aggregate counts for each concert and piece: first date of performance, # times performed\n",
    "    sso_df_copy = sso_df_copy.groupby(['Key', 'Concert', 'Piece', 'Composer', 'ComposerNationality', 'ComposerTimeline', 'ComposerStatus', 'ComposerGender', 'Conductor', 'ConductorNationality', 'ConductorGender', 'Artist_Metadata'])['Date'].agg([lambda date: datetime.strptime(date.min(), '%Y-%m-%d %H:%M').strftime('%Y-%m-%d'), lambda date: date.count()]).rename(columns={'<lambda_0>': 'First Date', '<lambda_1>': '# Performances'}).reset_index().sort_values(by=['First Date', 'Key'])\n",
    "\n",
    "    # re-arrange the columns\n",
    "    ordered_cols = ['Key', 'Concert', 'First Date', '# Performances', 'Piece', 'Composer', 'ComposerNationality', 'ComposerTimeline', 'ComposerStatus', 'ComposerGender', 'Conductor', 'ConductorNationality', 'ConductorGender', 'Artist_Metadata']\n",
    "    sso_df_copy = sso_df_copy[ordered_cols]\n",
    "\n",
    "    # convert Artist_Metadata back into a list of tuples\n",
    "    sso_df_copy['Artist_Metadata'] = sso_df_copy['Artist_Metadata'].apply(lambda x: eval(x))\n",
    "\n",
    "    return sso_df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # check whether to use back- or forward-slash path separators, depending on platform (Windows or Unix-based)\n",
    "    if sys.platform in ['cygwin', 'win32']:\n",
    "        path_separator = '\\\\'\n",
    "    else:\n",
    "        path_separator = '/'\n",
    "    \n",
    "    SSO_PICKLE_FILE = \"sso_2018_2021_cleaned.pkl\"\n",
    "    WIKIPEDIA_MASTER_NATL_FILE = \"Wikipedia-Composers_by_nationality.xml\"\n",
    "    WIKIPEDIA_SUB_NATL_FILE = \"Wikipedia-Composers_by_sub_nationality.xml\"\n",
    "\n",
    "    COMPOSER_NATIONALITY_MAP_FILE = \"sso_composer_nationality_map.csv\"\n",
    "    CONDUCTOR_NATIONALITY_MAP_FILE = \"sso_conductor_nationality_map.csv\"\n",
    "    SSO_OUTFILE_PREFIX = \"sso_2018_2021_cleaned_merged\"\n",
    "\n",
    "    # 1. load SSO pickle file into a DataFrame\n",
    "    sso_cleaned_df = file_utils.ProcessPickle().load_pickle(f\"data{path_separator}{SSO_PICKLE_FILE}\")\n",
    "    print(f\"{SSO_PICKLE_FILE} dimensions: {sso_cleaned.shape}\")\n",
    "\n",
    "    \"\"\"\n",
    "    ### (BEGIN) EXECUTE SECTION ONLY IF COMPOSER/CONDUCTOR NATIONALITY MAP FILES STILL NEED TO BE GENERATED (BEGIN) ###\n",
    "    # 2a. (if needed) import and merge Wikipedia data\n",
    "    wikipedia_df = pd.DataFrame(parse_composer_nationalities(WIKIPEDIA_MASTER_NATL_FILE, WIKIPEDIA_SUB_NATL_FILE, 1, 1))\n",
    "    wikipedia_df = wiki_remove_nationality_duplicates(wikipedia_df)\n",
    "    wikipedia_df = wiki_clean_nationality_data(wikipedia_df)\n",
    "\n",
    "    # 2b. (optional) uncomment the following line to write wikipedia_df to a Pickle file\n",
    "    #wikipedia_df.to_pickle(f\"data{path_separator}wikipedia-composers_by_nationality.pkl\")\n",
    "    #print(f\"Wrote Pickle file: data{path_separator}wikipedia-composers_by_nationality.pkl\")\n",
    "\n",
    "    ## If COMPOSER_NATIONALITY_MAP_FILE does not already exist, uncomment the 2nd line to generate it\n",
    "    # 3. (if needed) generate composer nationality map CSV\n",
    "    composer_nationality_map = WikidataUtils.merge_wikidata_and_sso_composers(sso_cleaned_df, wikipedia_df, export=1)\n",
    "\n",
    "    ## If CONDUCTOR_NATIONALITY_MAP_FILE does not already exist, uncomment the 2nd line to generate it\n",
    "    # 4. (if needed) generate conductor nationality map CSV\n",
    "    conductor_nationality_map = WikidataUtils.merge_wikidata_and_sso_conductors(sso_cleaned_df, export=1)\n",
    "    ### (END) EXECUTE SECTION ONLY IF COMPOSER/CONDUCTOR NATIONALITY MAP FILES STILL NEED TO BE GENERATED (END) ###\n",
    "    \"\"\"\n",
    "\n",
    "    # 5. join cleaned SSO data and conductor and composer metadata\n",
    "    sso_cleaned_merged_df = merge_conductor_composer_metadata(sso_cleaned_df, f\"data{path_separator}{CONDUCTOR_NATIONALITY_MAP_FILE}\", f\"data{path_separator}{COMPOSER_NATIONALITY_MAP_FILE}\")\n",
    "    print(f\"\\nsso_cleaned_merged dimensions: {sso_cleaned_merged_df.shape}\\n\")\n",
    "    print(sso_cleaned_merged_df.head(5))\n",
    "\n",
    "    # 6. write merged data to CSV\n",
    "    sso_cleaned_merged_df.to_pickle(f\"data{path_separator}{SSO_OUTFILE_PREFIX}.pkl\")\n",
    "    print(f\"\\nWrote Pickle file: data{path_separator}{SSO_OUTFILE_PREFIX}.pkl\")\n",
    "    sso_cleaned_merged_df.to_csv(path_or_buf=f\"data{path_separator}{SSO_OUTFILE_PREFIX}.csv\", index=False)\n",
    "    print(f\"Wrote CSV file: data{path_separator}{SSO_OUTFILE_PREFIX}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}